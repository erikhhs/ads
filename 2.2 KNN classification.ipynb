{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest neighbours classification\n",
    "## Instructions:\n",
    "* Go through the notebook and complete the tasks. \n",
    "* Make sure you understand the examples given. If you need help, refer to the Essential readings or the documentation link provided, or go to the Topic 2 discussion forum. \n",
    "* When a question allows a free-form answer (e.g. what do you observe?), create a new markdown cell below and answer the question in the notebook. \n",
    "* Save your notebooks when you are done.\n",
    " \n",
    "**Task 1:**\n",
    "Run the cell below to load our data. Notice the last line, where we add some random Gaussian noise to our data to make the task more challenging (data in real life usually contains some form of noise).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#view a description of the dataset \n",
    "print(iris.DESCR)\n",
    "\n",
    "#Set X a samples times features matrix, Y equal to the targets\n",
    "X=iris.data \n",
    "y=iris.target \n",
    "\n",
    "\n",
    "#we add some random noise to our data to make the task more challenging\n",
    "X=X+np.random.normal(0,0.4,X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:**\n",
    "1.\tHow many data samples do we have?\n",
    "2.\tPrint the value below using shape on ```X``` appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "#Enter code here\n",
    "X.shape\n",
    "print(X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:**\n",
    "1.\tHow many features do we have?\n",
    "2.\tPrint the value below using shape on ```X``` appropriately. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#Enter code here\n",
    "print(X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:**\n",
    "1.\tHow many classes do we have?\n",
    "2.\tPrint the value below using ```np.unique``` appropriately. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#Enter code here\n",
    "print(len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5:**\n",
    "1.\tHow many samples do we have that belong to class 1?\n",
    "2.\tPrint this in the cell below using the ```np.where``` function appropriately. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "#Enter code here\n",
    "ones = np.where(y==1)\n",
    "print(len(ones[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6:** \n",
    "\n",
    "Assume we want to generate a list of shuffled indices of our data. Use the function ```numpy.random.permutation``` to do that. In the cell below, you can already see how to create a list of indices that is not shuffled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 104, 107, 132, 136, 49, 58, 108, 13, 106, 117, 120, 12, 20, 6, 31, 37, 124, 15, 59, 86, 114, 5, 77, 44, 17, 105, 22, 9, 11, 39, 149, 94, 36, 72, 18, 109, 147, 0, 80, 8, 73, 57, 55, 122, 70, 90, 112, 53, 115, 7, 148, 61, 54, 24, 99, 66, 27, 81, 10, 138, 96, 79, 85, 123, 128, 103, 127, 113, 52, 131, 121, 134, 41, 4, 118, 68, 51, 100, 116, 62, 91, 29, 78, 75, 28, 87, 141, 111, 34, 3, 137, 65, 139, 74, 144, 83, 102, 146, 140, 60, 56, 63, 130, 97, 19, 33, 1, 126, 2, 42, 21, 67, 133, 76, 125, 95, 30, 71, 64, 142, 26, 98, 143, 145, 69, 93, 32, 92, 119, 35, 82, 88, 16, 47, 135, 14, 23, 45, 38, 84, 46, 43, 50, 129, 89, 110, 40, 101, 25]\n"
     ]
    }
   ],
   "source": [
    "#L=list(range(X.shape[0]))\n",
    "#print(L)\n",
    "#Enter code here\n",
    "L1 = list(np.random.permutation(range(X.shape[0])))\n",
    "print(L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7:**\n",
    "Here is an example of using the k-NN classifier. We split our data to training and testing (with a 0.2 percentage for our test data), fit on the training data, test on the testing data. \n",
    "Go through the code and make sure you understand it.\n",
    "Now do the same for the next cell, which prints the confusion matrix and the total accuracy. \n",
    "You can find some documentation to help you here: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html. \n",
    "Note that for this lab, we use the Euclidean distance along with 10 neighbours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 1 0 0 0 0 1 1 0 2 1 1 1 2 2 1 1 2 2 2 0 0 1 0 2 0 2 1 0]\n",
      "[1 2 1 0 0 0 0 1 1 0 1 1 1 1 2 2 1 1 2 2 2 0 0 1 0 2 0 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#split to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "#define knn classifier, with 5 neighbors and use the euclidian distance\n",
    "knn=KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "#define training and testing data, fit the classifier\n",
    "knn.fit(X_train,y_train)\n",
    "#predict values for test data based on training data\n",
    "y_pred=knn.predict(X_test)\n",
    "#print values\n",
    "print(y_test) # true values\n",
    "print(y_pred) # predicted values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  2  8]]\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8:**\n",
    "Write your <b>own</b> functions that return the confusion matrix given the true and predicted labels, as well as the accuracy. To do so, fill in the code in the next two cells. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  2  8]]\n"
     ]
    }
   ],
   "source": [
    "# model answer\n",
    "\n",
    "#create a matrix with entries equal to zero, and subsequently build the confusion matrix\n",
    "#the method should return the confusion matrix in a numpy array\n",
    "def myConfMat(y_ground,y_pred,classno):\n",
    "    C= np.zeros((classno,classno),dtype=np.int)\n",
    "    for i in range(0,len(y_test)):\n",
    "            \n",
    "    #complete this line - assign the appropriate value to C[i,j]\n",
    "           \n",
    "        C[y_ground[i],y_pred[i]]+=1\n",
    "    return C\n",
    "\n",
    "#note: len(np.unique(y))  indicates the dimensions of the confusion matrix (why?)\n",
    "print(myConfMat(y_test,y_pred,len(np.unique(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same = np.where(y_test == y_pred)\n",
    "len(same[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "# model answers\n",
    "\n",
    "#use the numpy function where to return the accuracy given the true/predicted labels.  i.e., #correct/#total\n",
    "def myAccuracy(y_test,y_pred):\n",
    "    same = np.where(y_test == y_pred)\n",
    "    A = len(same[0]) / len(y_test)      \n",
    "    return A\n",
    "    \n",
    "    \n",
    "print(myAccuracy(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional task:**</span> Write your own functions to calculate class-relative precision and recall. Compare these to the sklearn functions ``precision_score`` and ``recall_score`` on your y_test and y_pred values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_test)\n",
    "C = myConfMat(y_test,y_pred,len(np.unique(y)))\n",
    "for i in range(0, len(classes)):\n",
    "    tp = C[i,i]\n",
    "    col = sum(C[:,i])\n",
    "    prec = tp / col\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:      [0 1 2]\n",
      "my precision: [1.         0.81818182 0.88888889]\n",
      "my recall:    [1.  0.9 0.8]\n"
     ]
    }
   ],
   "source": [
    "#hint: you can use the output from your myConfMat function above\n",
    "\n",
    "def myPrecision(y_ground,y_pred):\n",
    "    classes = np.unique(y_ground)\n",
    "    precision = np.zeros(classes.shape)\n",
    "    for i in range(0, len(classes)):\n",
    "        tp = C[i,i]\n",
    "        col = sum(C[:,i])\n",
    "        precision[i] = tp / col        \n",
    "    return precision\n",
    "\n",
    "\n",
    "def myRecall(y_test,y_pred):\n",
    "    classes = np.unique(y_pred)\n",
    "    recall = np.zeros(classes.shape) \n",
    "    for i in range(0, len(classes)):\n",
    "        tp = C[i,i]\n",
    "        row = sum(C[i,:])\n",
    "        recall[i] = tp / row        \n",
    "    return recall\n",
    "\n",
    "print('classes:      %s' % np.unique(y_pred) )    \n",
    "print('my precision: %s' % myPrecision(y_test,y_pred))\n",
    "print('my recall:    %s' % myRecall(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "library precision: [1.         0.81818182 0.88888889]\n",
      "library recall: [1.  0.9 0.8]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score \n",
    "# check that your functions do the same thing as the library versions\n",
    "\n",
    "print('library precision: %s' % precision_score(y_test,y_pred,average=None))\n",
    "print('library recall: %s' % recall_score(y_test,y_pred,average=None))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
